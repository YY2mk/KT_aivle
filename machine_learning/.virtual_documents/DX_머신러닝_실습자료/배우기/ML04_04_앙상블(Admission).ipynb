











# 라이브러리 불러오기
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import warnings

warnings.filterwarnings(action='ignore')
%config InlineBackend.figure_format = 'retina'


# 데이터 불러오기
path = 'https://raw.githubusercontent.com/jangrae/csv/master/admission_simple.csv'
data = pd.read_csv(path)





# 데이터 살펴보기
data.head()


# 기술통계 확인
data.describe()








# target 확인
target = 'ADMIT'

# 데이터 분리
x = data.drop(target, axis=1)
y = data[target]





# 모듈 불러오기
from sklearn.model_selection import train_test_split

# 7:3으로 분리
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=1)





# 모듈 불러오기
from sklearn.preprocessing import MinMaxScaler

# 정규화
scaler = MinMaxScaler()
scaler.fit(x_train)
x_train_s = scaler.transform(x_train)
x_test_s = scaler.transform(x_test)





# xgboost 설치
# !pip install xgboost


# lightgbm 설치
# !pip install lightgbm


# 라이브러리 불러오기
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier
from lightgbm import LGBMClassifier

from sklearn.metrics import *





# 선언하기
model = KNeighborsClassifier(n_neighbors=5)


# 학습하기
model.fit(x_train_s, y_train)


# 예측하기
y_pred = model.predict(x_test_s)


# 평가하기
print(confusion_matrix(y_test, y_pred))
print(classification_report(y_test, y_pred))





# 선언하기
model = DecisionTreeClassifier(max_depth=5, random_state=1)


# 학습하기
model.fit(x_train, y_train)


# 예측하기
y_pred = model.predict(x_test)


# 5단계: 평가하기
print(confusion_matrix(y_test, y_pred))
print(classification_report(y_test, y_pred))





# 선언하기
model = LogisticRegression()


# 학습하기
model.fit(x_train, y_train)


# 예측하기
y_pred = model.predict(x_test)


# 5단계: 평가하기
print(confusion_matrix(y_test, y_pred))
print(classification_report(y_test, y_pred))





# 선언하기
model = RandomForestClassifier(max_depth=5, n_estimators=100) # 깊이를 5로 통일


# 학습하기
model.fit(x_train, y_train)


# 예측하기
y_pred = model.predict(x_test)


# 5단계: 평가하기
print(confusion_matrix(y_test, y_pred))
print(classification_report(y_test, y_pred))


# Feature 중요도 확인
plt.barh(y=list(x), width=model.feature_importances_)
plt.show()





# 선언하기
model = XGBClassifier(max_depth=5, n_estimators=100)


# 학습하기
model.fit(x_train, y_train)


# 예측하기
y_pred = model.predict(x_test)


# 평가하기
print(confusion_matrix(y_test, y_pred))
print(classification_report(y_test, y_pred))


# Feature 중요도 확인
plt.barh(y=list(x), width=model.feature_importances_)
plt.show()





# 선언하기
model = LGBMClassifier(max_depth=5, importance_type='gain', verbose=-1)


# 학습하기
model.fit(x_train, y_train)


# 예측하기
y_pred = model.predict(x_test)


# 평가하기
print(confusion_matrix(y_test, y_pred))
print(classification_report(y_test, y_pred))


# Feature 중요도 확인
plt.barh(y=list(x), width=model.feature_importances_)
plt.show()


# 0 ~ 1 사이 값으로 변환
feature_importance_norm = model.feature_importances_/ np.sum(model.feature_importances_)
plt.barh(y=list(x), width=feature_importance_norm)
plt.show()



