











# 라이브러리 불러오기
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import warnings

warnings.filterwarnings(action='ignore')
%config InlineBackend.figure_format = 'retina'


# 학습 데이터 불러오기
path = 'https://raw.githubusercontent.com/jangrae/csv/master/insurance_train.csv'
data1 = pd.read_csv(path)

# 평가 데이터 불러오기
path = 'https://raw.githubusercontent.com/jangrae/csv/master/insurance_test.csv'
data2 = pd.read_csv(path)





# 학습 데이터 살펴보기
data1.head()





# 평가 데이터 살펴보기
data2.head()


# 학습 데이터 기술통계 확인
data1.describe().T


# 평가 데이터 기술통계 확인
data2.describe().T


# 학습 데이터 Target 분포 시각화
sns.histplot(data=data1, x='charges', kde=True)
plt.show()


# 학습 데이터 흡연자 비율 시각화
sns.countplot(data=data1, x='smoker')  
plt.show()


# 학습 데이터 남녀 비율 시각화
sns.countplot(data=data1, x='sex')
plt.show()


# 학습 데이터 흡연 여부에 따른 의료비 시각화
sns.histplot(data=data1, x='charges', hue='smoker')
plt.show()








# target 확인
target = 'charges'

# 데이터 분리
x = data1.drop(target, axis=1)
y = data1.loc[:, target]


data1.head()





# 가변수화 대상
dumm_cols = ['sex', 'smoker', 'region']

# 가변수화
x = pd.get_dummies(x, columns=dumm_cols, drop_first=True, dtype=int)

# 확인
x





# 모듈 불러오기
from sklearn.model_selection import train_test_split

# 데이터 분리
x_train, x_val, y_train, y_val = train_test_split(x, y, test_size=0.3, random_state=1)





# 모듈 불러오기
from sklearn.preprocessing import MinMaxScaler

# 정규화
scaler = MinMaxScaler()
scaler.fit(x_train)
x_train_s = scaler.transform(x_train)
x_val_s = scaler.transform(x_val)





# xgboost 설치
# !pip install xgboost


# lightgbm 설치
# !pip install lightgbm





# 불러오기
from sklearn.neighbors import KNeighborsRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor
from xgboost import XGBRegressor

from sklearn.model_selection import GridSearchCV
from sklearn.metrics import *





# 매개변수 범위('n_neighbors': range(1, 21))
param = {'n_neighbors': range(1, 21)}

# 모델 선언
model_knn = KNeighborsRegressor()


# 매개변수와 성능 확인
model_knn = GridSearchCV(model_knn,
                     param,
                     cv=5,
                     scoring='r2'
                     )

# 학습
model_knn.fit(x_train_s, y_train)

print(model_knn.best_params_)
print(model_knn.best_score_)





# 예측하기
y_val_pred = model_knn.predict(x_val_s)

# 검증하기
print(mean_absolute_error(y_val, y_val_pred))
print(r2_score(y_val, y_val_pred))


# 성능정보 수집(R2 Score)
result = {}
result['KNN'] = r2_score(y_val, y_val_pred)
print(result)





# 매개변수 범위('max_depth': range(1, 11))
param = {'max_depth': range(1, 21)}

# 모델 선언
model_dt = DecisionTreeRegressor()

# 매개변수와 성능 확인
model_dt = GridSearchCV(model_dt,
                     param,
                     cv=5,
                     scoring='r2'
                     )

# 학습
model_dt.fit(x_train, y_train)

print(model_dt.best_params_)
print(model_dt.best_score_)





# 예측하기
y_val_pred = model_dt.predict(x_val)

# 검증하기
print(mean_absolute_error(y_val, y_val_pred))
print(r2_score(y_val, y_val_pred))

# 성능정보 수집(R2 Score)
result['Decision Tree'] = r2_score(y_val, y_val_pred)
print(result)





# 매개변수 범위('max_depth': range(1, 11))
param = {'max_depth': range(1, 11)}

# 모델 선언
model_rf = RandomForestRegressor()

# 매개변수와 성능 확인
model_rf = GridSearchCV(model_rf,
                     param,
                     cv=5,
                     scoring='r2'
                     )

# 학습
model_rf.fit(x_train, y_train)

print(model_dt.best_params_)
print(model_dt.best_score_)





# 예측하기
y_val_pred = model_rf.predict(x_val)

# 검증하기
print(mean_absolute_error(y_val, y_val_pred))
print(r2_score(y_val, y_val_pred))

# 성능정보 수집(R2 Score)
result['Random Forest'] = r2_score(y_val, y_val_pred)
print(result)





# 매개변수 범위('max_depth': range(1, 11))
param = {'max_depth': range(1, 11)}

# 모델 선언
model_xgb = XGBRegressor()

# 매개변수와 성능 확인
model_xgb = GridSearchCV(model_xgb,
                     param,
                     cv=5,
                     scoring='r2'
                     )

# 학습
model_xgb.fit(x_train, y_train)

print(model_xgb.best_params_)
print(model_xgb.best_score_)





# 예측하기
y_val_pred = model_rf.predict(x_val)

# 검증하기
print(mean_absolute_error(y_val, y_val_pred))
print(r2_score(y_val, y_val_pred))

# 성능정보 수집(R2 Score)
result['XGBoost'] = r2_score(y_val, y_val_pred)
print(result)





# 성능 비교
print('=' * 40)
for m_name, score in result.items():
    print(m_name, score.round(3))
print('=' * 40)


# 데이터프레임 만들기
df = pd.DataFrame()
df['name'] = result.keys()
df['R2'] = result.values()

# 확인
df












# 평가 데이터
x_test = data2

# 확인
x_test.head()





# 가변수화 대상
dumm_cols = ['sex', 'smoker', 'region']

# 가변수화
x_test = pd.get_dummies(x_test, columns=dumm_cols, drop_first=True, dtype=int)

# 확인
x_test





# 예측하기
y_pred = model_dt.predict(x_test)


# 예측값 확인
print(mean_absolute_error(y_val, y_pred))
print(r2_score(y_val, y_pred))





# 예측 결과 병합
data2['prd_charges'] = y_pred

# 롹인
data2.head()


# 파일 저장
data2.to_excel('insurance_predicted.xlsx', index=False)



