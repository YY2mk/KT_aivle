











# 라이브러리 불러오기
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import warnings

warnings.filterwarnings(action='ignore')
%config InlineBackend.figure_format = 'retina'


# 데이터 불러오기
path = 'https://raw.githubusercontent.com/jangrae/csv/master/mobile_cust_churn.csv'
data = pd.read_csv(path)
data['CHURN'] = data['CHURN'].map({'STAY':0, 'LEAVE': 1})





# 데이터 살펴보기
data.head()





# 기술통계 확인
data.describe()








# 제거 대상: id
drop_cols = ['id']

# 변수 제거
data = data.drop(drop_cols, axis=1)

# 확인
data





# Target 설정
target = 'CHURN'

# 데이터 분리
x = data.drop(target, axis=1)
y = data.loc[:, target]





# 가변수화 대상
dumm_cols = ['REPORTED_SATISFACTION', 'REPORTED_USAGE_LEVEL', 'CONSIDERING_CHANGE_OF_PLAN']

# 가변수화
x = pd.get_dummies(x, columns=dumm_cols, drop_first=True, dtype=int)

# 확인
x





# 모듈 불러오기
from sklearn.model_selection import train_test_split

# 7:3으로 분리
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=1)





# 모듈 불러오기
from sklearn.preprocessing import MinMaxScaler

# 정규화
scaler = MinMaxScaler()
scaler.fit(x_train)
x_train_s = scaler.transform(x_train)
x_test_s = scaler.transform(x_test)





# xgboost 설치
# !pip install xgboost


# lightgbm 설치
# !pip install lightgbm





# 불러오기
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier
from lightgbm import LGBMClassifier

from sklearn.model_selection import cross_val_score, GridSearchCV
from sklearn.metrics import *





# 선언하기
model_knn = KNeighborsClassifier()


# 성능예측
cv_score = cross_val_score(model_knn, x_train_s, y_train, cv=10)


# 결과확인
print(cv_score)
print(cv_score.mean())


# 결과수집
result = {}
result['KNN'] = cv_score.mean()





# 선언하기
model_dt = DecisionTreeClassifier()


# 성능예측
cv_score = cross_val_score(model_dt, x_train, y_train, cv=10)


# 결과확인
print(cv_score)
print(cv_score.mean())


# 결과수집
result['Decision Tree'] = cv_score.mean()





# 선언하기
model_lr = LogisticRegression()


# 성능예측
cv_score = cross_val_score(model_lr, x_train, y_train, cv=10)


# 결과확인
print(cv_score)
print(cv_score.mean())


# 결과수집
result['Logistic Regression'] = cv_score.mean()





# 선언하기
model_rf = RandomForestClassifier()


# 성능예측
cv_score = cross_val_score(model_rf, x_train, y_train, cv=10)


# 결과확인
print(cv_score)
print(cv_score.mean())


# 결과수집
result['Random Forest'] = cv_score.mean()





# 선언하기
model_xgb = XGBClassifier()


# 성능예측
cv_score = cross_val_score(model_xgb, x_train, y_train, cv=10)


# 결과확인
print(cv_score)
print(cv_score.mean())


# 결과수집
result['XGBoost'] = cv_score.mean()





# 선언하기
model_lgbm = LGBMClassifier(max_depth=5, importance_type='gain',  verbose=-1)


# 성능예측
cv_score = cross_val_score(model_lgbm, x_train, y_train, cv=10)


# 결과확인
print(cv_score)
print(cv_score.mean())


# 결과수집
result['LightGBM'] = cv_score.mean()





# 성능 비교
print('=' * 40)
for m_name, score in result.items():
    print(m_name, score.round(3))
print('=' * 40)





# 기본 모델 선언
model = LGBMClassifier()

# 파라미터 지정
  # max_depth: range(1, 21)
param = {'max_depth': range(1, 21)}

# 모델 선언
model = GridSearchCV(model,
                     param,
                     cv=5,
                     scoring='accuracy'
                     )


# 학습하기(많은 시간이 소요될 수 있음)
model.fit(x_train, y_train)


# 최적 파라미터, 예측 최고 성능
print(model.best_params_)
print(model.best_score_)


# 변수 중요도 시각화
plt.barh(y=list(x), width=model.best_estimator_.feature_importances_)
plt.show()





# 예측하기
y_pred = model.predict(x_test)


# 성능평가
print(confusion_matrix(y_test, y_pred))
print(classification_report(y_test, y_pred))



