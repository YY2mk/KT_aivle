




















path = 'C:/Users/User/program/mini_pjt/mini_3/'








# from google.colab import drive
# drive.mount('/content/drive')


# path = '/content/drive/MyDrive/project/'








import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# 필요하다고 판단되는 라이브러리를 추가하세요.
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import GridSearchCV
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import *





# 변수의 특성 중요도 계산하기
def plot_feature_importance(importance, names, result_only = False, topn = 'all'):
    feature_importance = np.array(importance)
    feature_name = np.array(names)

    data={'feature_name':feature_name,'feature_importance':feature_importance}
    fi_temp = pd.DataFrame(data)

    #변수의 특성 중요도 순으로 정렬하기
    fi_temp.sort_values(by=['feature_importance'], ascending=False,inplace=True)
    fi_temp.reset_index(drop=True, inplace = True)

    if topn == 'all' :
        fi_df = fi_temp.copy()
    else :
        fi_df = fi_temp.iloc[:topn]

    #변수의 특성 중요도 그래프로 그리기
    if result_only == False :
        plt.figure(figsize=(10,20))
        sns.barplot(x='feature_importance', y='feature_name', data = fi_df)

        plt.xlabel('importance')
        plt.ylabel('feature name')
        plt.grid()

    return fi_df











train = pd.read_csv('data01_train.csv')
test = pd.read_csv('data01_test.csv')
feature = pd.read_csv('features.csv')


train.drop('subject', axis=1, inplace=True)
test.drop('subject', axis=1, inplace=True)


train.head()


test.head()





train.describe()


train.info()


train.shape


test.describe()


test.info()


test.shape








train['Activity_dynamic'] = train['Activity'].map({'STANDING':0, 'SITTING':0, 'LAYING':0, 
                  'WALKING':1, 'WALKING_UPSTAIRS':1, 'WALKING_DOWNSTAIRS':1})
train


target1 = 'Activity'
target2 = 'Activity_dynamic'

x = train.drop(target1, axis=1)
y1 = train.loc[:, target1]
x = x.drop(target2, axis=1)
y2 = train.loc[:, target2]

x_train, x_val, y_train, y_val = train_test_split(x, y1, test_size=0.2, random_state=2024)
x_train, x_val, y_train2, y_val2 = train_test_split(x, y2, test_size=0.2, random_state=2024)

















model_lr = LogisticRegression()


model_lr.fit(x_train, y_train2)


y_pred_1 = model_lr.predict(x_val)


# 평가
print('accuracy :',accuracy_score(y_val2, y_pred_1))
print('='*60)
print(confusion_matrix(y_val2, y_pred_1))
print('='*60)
print(classification_report(y_val2, y_pred_1))





model_rf = RandomForestClassifier()
model_rf.fit(x_train, y_train2)
y_pred_2 = model_rf.predict(x_val)


# 평가
print('accuracy :',accuracy_score(y_val2, y_pred_2))
print('='*60)
print(confusion_matrix(y_val2, y_pred_2))
print('='*60)
print(classification_report(y_val2, y_pred_2))








s_data = train.loc[train['Activity_dynamic'] == 0]
target = 'Activity'
s_x = s_data.drop(target, axis=1)
s_y = s_data.loc[:, target]

i_train, i_val, t_train, t_val = train_test_split(s_x, s_y, test_size=0.3, random_state=2024)


model_lr2 = LogisticRegression()
model_lr2.fit(i_train, t_train)
s_pred = model_lr2.predict(i_val)

# 평가
print('accuracy :',accuracy_score(t_val, s_pred))
print('='*60)
print(confusion_matrix(t_val, s_pred))
print('='*60)
print(classification_report(t_val, s_pred))


model_rf2 = RandomForestClassifier()
model_rf2.fit(i_train, t_train) 
s_pred = model_rf2.predict(i_val)

# 평가
print('accuracy :',accuracy_score(t_val, s_pred))
print('='*60)
print(confusion_matrix(t_val, s_pred))
print('='*60)
print(classification_report(t_val, s_pred))


model_lr2 = LogisticRegression()
model_lr2.fit(x_train, y_train)
s_pred = model_lr2.predict(x_val)

# 평가
print('accuracy :',accuracy_score(y_val, s_pred))
print('='*60)
print(confusion_matrix(y_val, s_pred))
print('='*60)
print(classification_report(y_val, s_pred))


model_rf2 = RandomForestClassifier()
model_rf2.fit(x_train, y_train)
s_pred = model_rf2.predict(x_val)

# 평가
print('accuracy :',accuracy_score(y_val, s_pred))
print('='*60)
print(confusion_matrix(y_val, s_pred))
print('='*60)
print(classification_report(y_val, s_pred))











s_data = train.loc[train['Activity_dynamic'] == 1]
target = 'Activity'
s_x = s_data.drop(target, axis=1)
s_y = s_data.loc[:, target]

i_train, i_val, t_train, t_val = train_test_split(s_x, s_y, test_size=0.3, random_state=2024)


model_lr2 = LogisticRegression()
model_lr2.fit(x_train, y_train) 
s_pred = model_lr2.predict(x_val)

# 평가
print('accuracy :',accuracy_score(y_val, s_pred))
print('='*60)
print(confusion_matrix(y_val, s_pred))
print('='*60)
print(classification_report(y_val, s_pred))


model_rf2 = RandomForestClassifier()
model_rf2.fit(x_train, y_train) 
s_pred = model_rf2.predict(x_val)

# 평가
print('accuracy :',accuracy_score(y_val, s_pred))
print('='*60)
print(confusion_matrix(y_val, s_pred))
print('='*60)
print(classification_report(y_val, s_pred))














def evaluate_second_target(train, target1, target2, test_size=0.2, random_state=2024):
    """
    주어진 데이터에 대해 두 번째 타겟 변수에 대한 두 가지 모델을 평가하는 함수
    
    - train (DataFrame): 학습에 사용할 데이터셋
    - target1 (str): 첫 번째 타겟 변수 이름
    - target2 (str): 두 번째 타겟 변수 이름
    - test_size (float, optional): 테스트 데이터셋의 비율 (기본값: 0.2)
    - random_state (int, optional): 랜덤 시드 값 (기본값: 2024)
    """
    # 특징 데이터(x)와 타겟 데이터(y) 설정
    x = train.drop([target1, target2], axis=1)
    y1 = train[target1]
    y2 = train[target2]
    
    # 데이터 분할
    x_train, x_val, y_train1, y_val1 = train_test_split(x, y1, test_size=test_size, random_state=random_state)
    x_train, x_val, y_train2, y_val2 = train_test_split(x, y2, test_size=test_size, random_state=random_state)

    # 모델 학습 (로지스틱 회귀)
    model_lr = LogisticRegression()
    model_lr.fit(x_train, y_train2)
    
    # 로지스틱 회귀 모델 평가
    y_pred_lr = model_lr.predict(x_val)
    print("Logistic Regression 모델 평가 결과:")
    print('accuracy :', accuracy_score(y_val2, y_pred_lr))
    print('='*60)
    print(confusion_matrix(y_val2, y_pred_lr))
    print('='*60)
    print(classification_report(y_val2, y_pred_lr))
    print('='*60)
    
    # 모델 학습 (랜덤 포레스트)
    model_rf = RandomForestClassifier(random_state=random_state)
    model_rf.fit(x_train, y_train1)
    
    # 랜덤 포레스트 모델 평가
    y_pred_rf = model_rf.predict(x_val)
    print("Random Forest 모델 평가 결과:")
    print('accuracy :', accuracy_score(y_val1, y_pred_rf))
    print('='*60)
    print(confusion_matrix(y_val1, y_pred_rf))
    print('='*60)
    print(classification_report(y_val1, y_pred_rf))
    print('='*60)


evaluate_second_target(train, 'Activity', 'Activity_dynamic')





def evaluate_second_target(train, target1, target2, test_size=0.2, random_state=2024):
    """
    주어진 데이터에 대해 두 번째 타겟 변수에 대한 두 가지 모델을 평가하고, 그리드 서치를 사용하여 최적의 파라미터를 찾아 출력하는 함수
    
    - train (DataFrame): 학습에 사용할 데이터셋
    - target1 (str): 첫 번째 타겟 변수 이름
    - target2 (str): 두 번째 타겟 변수 이름
    - test_size (float, optional): 테스트 데이터셋의 비율 (기본값: 0.2)
    - random_state (int, optional): 랜덤 시드 값 (기본값: 2024)
    """
    # 특징 데이터(x)와 타겟 데이터(y) 설정
    x = train.drop([target1, target2], axis=1)
    y1 = train[target1]
    y2 = train[target2]
    
    # 데이터 분할
    x_train, x_val, y_train1, y_val1 = train_test_split(x, y1, test_size=test_size, random_state=random_state)
    x_train, x_val, y_train2, y_val2 = train_test_split(x, y2, test_size=test_size, random_state=random_state)

    # 모델 학습 (로지스틱 회귀)
    param_grid_lr = {
        'n_estimators': [205, 220, 240],
        'max_depth' : [23, 30, 50, 60],
}
    grid_search_lr = GridSearchCV(RandomForestClassifier(), param_grid_lr, cv=5, scoring='accuracy')
    grid_search_lr.fit(x_train, y_train2)
    best_lr_model = grid_search_lr.best_estimator_
    best_lr_params = grid_search_lr.best_params_
    best_lr_score = grid_search_lr.best_score_
    
    # 로지스틱 회귀 모델 평가
    y_pred_lr = best_lr_model.predict(x_val)
    print("Logistic Regression 모델 평가 결과:")
    print('accuracy :', accuracy_score(y_val2, y_pred_lr))
    print('='*60)
    print(confusion_matrix(y_val2, y_pred_lr))
    print('='*60)
    print(classification_report(y_val2, y_pred_lr))
    print('='*60)
    print("최적의 파라미터:", best_lr_params)
    print("최고 성능:", best_lr_score)
    print('='*60)
    
    # 모델 학습 (랜덤 포레스트)
    param_grid_rf = {
        'n_estimators': [205, 220, 240],
        'max_depth' : [23, 30, 50, 60],
    }
    grid_search_rf = GridSearchCV(RandomForestClassifier(random_state=random_state), param_grid_rf, cv=5, scoring='accuracy')
    grid_search_rf.fit(x_train, y_train1)
    best_rf_model = grid_search_rf.best_estimator_
    best_rf_params = grid_search_rf.best_params_
    best_rf_score = grid_search_rf.best_score_
    
    # 랜덤 포레스트 모델 평가
    y_pred_rf = best_rf_model.predict(x_val)
    print("Random Forest 모델 평가 결과:")
    print('accuracy :', accuracy_score(y_val1, y_pred_rf))
    print('='*60)
    print(confusion_matrix(y_val1, y_pred_rf))
    print('='*60)
    print(classification_report(y_val1, y_pred_rf))
    print('='*60)
    print("최적의 파라미터:", best_rf_params)
    print("최고 성능:", best_rf_score)
    print('='*60)


# 함수 호출
evaluate_second_target(train, 'Activity', 'Activity_dynamic')






