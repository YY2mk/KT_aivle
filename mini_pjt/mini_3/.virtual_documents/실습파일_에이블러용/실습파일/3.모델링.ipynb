




















path = 'C:/Users/User/program/mini_pjt/mini_3/실습파일_에이블러용/데이터/'








# from google.colab import drive
# drive.mount('/content/drive')


#path = '/content/drive/MyDrive/project/'











#!pip install -r requirements.txt








import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

import joblib 

# 필요한 라이브러리 로딩
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.ensemble import RandomForestRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.linear_model import LinearRegression
from sklearn.neighbors import KNeighborsRegressor
from sklearn.metrics import *








def model_plot(y, pred) : 
    plt.figure(figsize = (12,8))
    plt.scatter(y, pred, alpha=0.4)

    x_l = np.linspace( y.min(), y.max(), 100)
    plt.plot(x_l, x_l, color = 'black', alpha=0.4)

    plt.xlabel("Actual")
    plt.ylabel("Predicted")
    plt.grid()
    plt.show()








train = joblib.load(path + 'train2.pkl')


train.head()




















train.columns


target = '실차량수'


train.drop('단지코드', axis=1, inplace=True)


x = train.drop(target, axis=1)
y = train.loc[:, target]





# 1번 데이터 전처리에서 조치함
train.isna().sum() 





train


drop_dumm = ['건물형태', '난방방식']
x = pd.get_dummies(x, columns=drop_dumm, drop_first=True, dtype=int)


x


drop_cols = ['30이하', '40이하',	'50이하', '60이하', '70이하', '80이하', '90이하', '100이하', '110이하', '120이하', '121이상']
x = x.drop(drop_cols, axis=1)
x





x_train, x_val, y_train, y_val = train_test_split(x, y, test_size=0.2, random_state=2024)





from sklearn.preprocessing import MinMaxScaler

scaler = MinMaxScaler()
scaler.fit(x_train)
x_train_s = scaler.transform(x_train)
x_val_s = scaler.transform(x_val)





model_lr = LinearRegression()
model_rf = RandomForestRegressor()
model_knn = KNeighborsRegressor()
model_dt = DecisionTreeRegressor()


# MAE : 115~178
# MAPE : 0.69~1.51
result = {}


def modle_mk(modle_select, x_train, y_train, x_val, y_val):
    if modle_select == model_rf:
        name = 'Random Forest'
        
        param = {
        'max_depth': [1, 5, 10, 20, 50],
        'n_estimators' : [10, 30, 50, 100, 150, 200]
        }
        model = GridSearchCV(modle_select,   # 튜닝할 기본 모델
                             param,       # 테스트 대상 매개변수 범위
                             cv=5,        # K-Fold cv 개수
                             scoring='r2' # 평가지표 (회귀여서 r2)
                             )
    
    if modle_select == model_lr:
        name = 'LinearRegression'
        model = modle_select
    
    if modle_select == model_knn:
        name = 'KNeighborsRegressor'
        
        param = {
            'n_neighbors': [3, 5, 7, 9, 11], #이웃의 수를 결정
            'weights': ['uniform', 'distance'], # uniform'으로 설정하면 모든 이웃에 동일한 가중치가 부여   # distance'로 설정하면 이웃간의 거리에 따라 가중치가 부여
            'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'], # 가장 가까운 이웃을 검색
            'p': [1, 2], # 맨하탄 거리('manhattan')(1)나 민코프스키 거리('minkowski')(2)를 사용
        }

        model = GridSearchCV(modle_select,   # 튜닝할 기본 모델
                                 param,       # 테스트 대상 매개변수 범위
                                 cv=5,        # K-Fold cv 개수
                                 scoring='r2' # 평가지표 (회귀여서 r2)
                                 )
    
    if modle_select == model_dt:
        name = 'DecisionTreeRegressor'

        param = {
        'max_depth': [1, 5, 10, 20, 50],
        'max_features': ['auto', 'sqrt', 'log2'] #각 노드에서 분할에 사용될 특성의 최대 수를 결정
                                                # 높은 값을 설정하면 더 다양한 특성을 고려할 수 있지만, 너무 높은 값은 모델이 더 복잡해지고 과적합의 위험이 있습니다
        }
        model = GridSearchCV(modle_select,   # 튜닝할 기본 모델
                             param,       # 테스트 대상 매개변수 범위
                             cv=5,        # K-Fold cv 개수
                             scoring='r2' # 평가지표 (회귀여서 r2)
                             )

    
    
    print("model_name:", name)
    model.fit(x_train, y_train)
    p1 = model.predict(x_val)
    
    print('MAE:', mean_absolute_error(y_val, p1)) # 낮을 수록 실제값에 가까움 # 낮을 수록 오차가 적음
    print('='*60)
    print('MSE:', mean_squared_error(y_val, p1)) # 낮을 수록 실제값에 가까움 #제곱 평균
    print('='*60)
    print('MSPE:', mean_absolute_percentage_error(y_val, p1)) # 낮을 수록 실제값에 가까움 #MSE의 제곱근
    print('='*60)
    print('R2:', r2_score(y_val, p1)) #결정계수 1에 가까울 수록 좋음
    print('='*60)
    print('최적의 파라미터:', model.best_params_)
    print('최고 성능:', model.best_score_)

    result[name] = model.best_score_
    return result


# Random Forest
modle_mk(model_rf, x_train, y_train, x_val, y_val) 





# DecisionTreeRegressor
modle_mk(model_dt, x_train, y_train, x_val, y_val) 





#KNeighborsRegressor
modle_mk(model_knn, x_train_s, y_train, x_val_s, y_val) 





# LinearRegression 
modle_mk(model_lr, x_train, y_train, x_val, y_val)








result











new_data = joblib.load(path + 'test3.pkl')


new_data





drop_cols = ['단지코드', '지역']
new_data.drop(drop_cols, axis=1, inplace=True)
new_y_val = new_data.loc[:, '실차량수']
new_data.drop('실차량수', axis=1, inplace=True)


new_data


dumm_cols = ['건물형태',	'난방방식']
new_data = pd.get_dummies(new_data, columns=dumm_cols, drop_first=True, dtype=int)
new_data


new_data


x_train





modle_mk(model_rf, x_train, y_train, new_data, new_y_val) 


model = RandomForestRegressor(max_depth=50, n_estimators=100)
model.fit(x_train, y_train)
y_pred = model.predict(new_data)
print('R2:', r2_score(new_y_val, y_pred))


modle_mk(model_lr, x_train, y_train, new_data, new_y_val) 


from sklearn.preprocessing import MinMaxScaler

scaler = MinMaxScaler()
scaler.fit(x_train)
x_train_s = scaler.transform(x_train)
new_val_s = scaler.transform(new_data)


modle_mk(model_knn, x_train_s, y_train, new_val_s, new_y_val)



